# MOEQ  
**MoQE: Improve Quantization Model Performance via Mixture of Quantization Experts**

<p align="center">
  <img alt="Python 3.10.9+" src="https://img.shields.io/badge/python-3.10.9+-blue.svg"/>
  <img alt="PyTorch 2.7+" src="https://img.shields.io/badge/PyTorch-2.7+-orange.svg"/>
  <img alt="License MIT" src="https://img.shields.io/badge/license-MIT-green.svg"/>
</p>

---

## ğŸ“Œ About
This repository provides an **efficient framework** for training **Mixture-of-Quantization-Experts (MoQE)** models that are built from **pre-trained, frozen, quantization experts**.  
The key idea is simple: keep all expert parameters frozen and only train a **lightweight gating network** that dynamically routes and combines the knowledge of these quantized experts .

| Advantages |
| --- |
| âœ… **Dramatically reduces training compute** |
| âœ… **Accelerates inference** |
| âœ… **Outperforms single quantized models** |

> NLP experiments were run on **A100 80 GB**; CV experiments on **V100S**.


## ğŸš€ Core Features

| Feature | Description |
| --- | --- |
| **Shallow-Sharing** | Unified embedding layer shared across all experts; expert bodies remain frozen â†’ minimal trainable params. |
| **Heterogeneous Experts** | Native support for **GGUF**, **Safetensors**, and moreâ€”mix-and-match open-source experts freely. |
| **Dynamic Gating (MoERouter)** | TransformerEncoder + self-attention router for **context-aware** routing decisions. |
| **Memory-Efficient Training** | â€¢ Gradient Checkpointing<br>â€¢ 8-bit AdamW (`bitsandbytes`) |
| **Smart GPU Allocation** | Auto VRAM profiling â†’ large experts go model-parallel, small ones land on the least-busy device. |
| **Curriculum Learning** | Start on a **smaller** dataset, then switch to a **larger** corpus for stable & efficient convergence. |

---
## ğŸ–¥ï¸ System Environment
| Component | Version |
| --- | --- |
| **Python** | 3.10.9 |
| **CUDA** | 12.2 |
| **GPU** | NVIDIA A100-SXM4-80GB |
| **Driver** | 535.54.03 |

## ğŸ“¦ Quick Setup
```bash
git clone https://best-jett/MOEQ.git

cd MOEQ

pip install -r requirements.txt

pip install torch==2.7.1 transformers==4.53.3 \
            bitsandbytes==0.47.0.dev0 pandas==2.3.1 \
            tqdm==4.67.1 accelerate==1.9.0
```
## Dependent library version See requirements.txt for the full list.


## ğŸ› ï¸ Usage
### 1ï¸âƒ£ Prepare Data & Models
- Put .parquet datasets in the specified directory.
- Prepare your quantized experts and note their paths.
- We use WikiText-2, OpenWebText, and C4 for experiments.

### 2ï¸âƒ£ Launch Training
- Basic Training
```bash
python train_model.py \
    --train \
    --expert_paths /path/to/expert1 /path/to/expert2 \
    --data_dir /path/to/data \
    --save_dir /path/to/save/checkpoints \
    --batch_size 8 \
    --gradient_accumulation_steps 6 \
    --learning_rate 5e-5 \
    --epochs 10
```
- Resume from Checkpoint
```bash
python train_model.py \
    --train \
    --checkpoint_path /path/to/save/checkpoints/checkpoint.pt \
    --expert_paths /path/to/expert1 /path/to/expert2 \
    --data_dir /path/to/data \
    --save_dir /path/to/save/checkpoints
```
âš™ï¸ Argument Reference
| Argument            | Default         | Description                     |
| ------------------- | --------------- | ------------------------------- |
| `--train`           | `False`         | Enable training mode            |
| `--eval`            | `False`         | Enable evaluation mode          |
| `--expert_paths`    | `None`          | Space-separated expert paths    |
| `--data_dir`        | `./data`        | Directory with `.parquet` files |
| `--save_dir`        | `./checkpoints` | Output directory                |
| `--from_scratch`    | `False`         | Ignore existing checkpoints     |
| `--checkpoint_path` | `None`          | Resume from checkpoint          |


# MOEQ
MoQE Training Code
# MoQE: Improve Quantization Model performance via Mixture of Quantization Experts.

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªç”¨äºæ··åˆé‡åŒ–ä¸“å®¶ï¼ˆMoQEï¼‰æ¨¡å‹çš„é«˜æ•ˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒã€å†»ç»“çš„ç»è¿‡é‡åŒ–çš„â€œä¸“å®¶â€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä»…è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„é—¨æ§ç½‘ç»œæ¥å­¦ä¹ å¦‚ä½•æ ¹æ®è¾“å…¥åŠ¨æ€åœ°è·¯ç”±å’Œç»„åˆè¿™äº›é‡åŒ–ä¸“å®¶çš„çŸ¥è¯†ã€‚

è¿™ç§æ–¹æ³•æ˜¾è‘—é™ä½äº†è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦å¹¶ä¸”æœ‰ç€æ¯”å•ä¸€çš„é‡åŒ–æ¨¡å‹æ›´åŠ å¼ºçš„æ€§èƒ½ã€‚è®ºæ–‡ä¸­çš„NLPå®éªŒåœ¨A100 80GBçš„æ˜¾å¡ä¸Šå®Œæˆï¼ŒCVå®éªŒåœ¨V100Sä¸Šå®Œæˆã€‚

## æ ¸å¿ƒç‰¹æ€§

- **æµ…å±‚å…±äº«æ¶æ„ (Shallow-Sharing)**: æ‰€æœ‰ä¸“å®¶æ¨¡å‹å…±äº«ä¸€ä¸ªç»Ÿä¸€çš„è¯åµŒå…¥å±‚ï¼Œè€Œä¸“å®¶è‡ªèº«çš„ä¸»ä½“å‚æ•°ä¿æŒå†»ç»“ï¼Œæå¤§åœ°å‡å°‘äº†å¯è®­ç»ƒå‚æ•°é‡ã€‚
- **å¼‚æ„ä¸“å®¶æ”¯æŒ (Heterogeneous Experts)**: æ¡†æ¶åŸç”Ÿæ”¯æŒåŠ è½½ä¸åŒæ ¼å¼çš„ä¸“å®¶æ¨¡å‹ï¼ŒåŒ…æ‹¬ GGUF å’Œ Safetensorsï¼Œå…è®¸çµæ´»ç»„åˆæ¥è‡ªå¼€æºç¤¾åŒºçš„å„ç±»æ¨¡å‹ã€‚
- **åŠ¨æ€é—¨æ§ç½‘ç»œ (Dynamic Gating Network)**: é‡‡ç”¨ä¸€ä¸ªåŒ…å«`TransformerEncoder`å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚`MoERouter`ï¼Œèƒ½å¤Ÿæ•æ‰è¾“å…¥åºåˆ—çš„æ·±å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åšå‡ºæ›´ç²¾å‡†çš„è·¯ç”±å†³ç­–ã€‚ã€‚
- **æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ (Memory-Efficient Training)**: é»˜è®¤å¯ç”¨**æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)**ã€**8-bit AdamW ä¼˜åŒ–å™¨**ä»¥åœ¨æ ‡å‡†å•å¡ä¸Šå®ç°å¤šä¸“å®¶æ¨¡å‹çš„ç¨³å®šè®­ç»ƒã€‚
- **æ™ºèƒ½GPUåˆ†é… (Smart GPU Allocation)**: è®­ç»ƒå‰è‡ªåŠ¨è¯„ä¼°ä¸“å®¶æ¨¡å‹çš„æ˜¾å­˜å ç”¨ï¼Œå¹¶å°†å¤§å‹æ¨¡å‹é…ç½®ä¸ºè·¨å¤šGPUçš„æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼Œå°å‹æ¨¡å‹åˆ™åˆ†é…è‡³å½“å‰æœ€ç©ºé—²çš„è®¾å¤‡ã€‚
- **è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (Curriculum Learning)**: æ”¯æŒåœ¨è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†ï¼Œåœ¨åæœŸåˆ‡æ¢åˆ°æ›´å¤§çš„æ•°æ®é›†ï¼Œä»¥å®ç°æ›´ç¨³å®šå’Œé«˜æ•ˆçš„æ”¶æ•›ã€‚

## ç¯å¢ƒè¦æ±‚

åœ¨è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹æ ¸å¿ƒä¾èµ–åº“ï¼š

```bash
cuda 12.2
pip install torch==2.7.1 transformers==4.53.3 bitsandbytes==0.47.0.dev0 pandas==2.3.1 tqdm==4.67.1 accelerate==1.9.0
```
æ›´å¤šè¯¦ç»†ä¿¡æ¯è§requirement.txt

## ä½¿ç”¨æ–¹æ³•

### 1. å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
- å°†æ•°æ®é›†ï¼ˆ`.parquet`æ ¼å¼ï¼‰æ”¾å…¥æŒ‡å®šçš„æ•°æ®ç›®å½•ã€‚
- å‡†å¤‡å¥½æ‚¨è¦ç”¨ä½œä¸“å®¶çš„æ¨¡å‹ï¼Œå¹¶è®°ä¸‹å®ƒä»¬çš„è·¯å¾„ã€‚
- æœ¬å®éªŒä½¿ç”¨çš„æ˜¯wikitext2,openwebtext,C4
### 2. å¼€å§‹è®­ç»ƒ
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒã€‚æ‚¨å¯ä»¥æ ¹æ®éœ€æ±‚ä¿®æ”¹å‘½ä»¤è¡Œå‚æ•°ã€‚

**åŸºç¡€è®­ç»ƒå‘½ä»¤ï¼š**
```bash
python train_model.py \
    --train \
    --expert_paths /path/to/expert1 /path/to/expert2 \
    --data_dir /path/to/your/data \
    --save_dir /path/to/save/checkpoints \
    --batch_size 8 \
    --gradient_accumulation_steps 6 \
    --learning_rate 5e-5 \
    --epochs 10
```

**ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼š**
```bash
python train_model.py \
    --train \
    --checkpoint_path /path/to/save/checkpoints/checkpoint.pt \
    --expert_paths /path/to/expert1 /path/to/expert2 \
    --data_dir /path/to/your/data \
    --save_dir /path/to/save/checkpoints
```

## å‚æ•°é…ç½®è¯¦è§£

ä»¥ä¸‹æ˜¯æ‰€æœ‰å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°åŠå…¶è¯´æ˜ã€‚

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|:---|:---:|:---|
| **æ ¸å¿ƒé…ç½®** | | |
| `--train` | `False` | å¯åŠ¨è®­ç»ƒæ¨¡å¼ã€‚ |
| `--eval` | `False` | å¯åŠ¨è¯„ä¼°æ¨¡å¼ã€‚ |
| `--expert_paths` | `None` | ä¸€ç³»åˆ—ä¸“å®¶æ¨¡å‹çš„è·¯å¾„ï¼Œä»¥ç©ºæ ¼åˆ†éš”ã€‚ |
| `--data_dir` | `/path/to/data` | åŒ…å«`.parquet`æ•°æ®é›†æ–‡ä»¶çš„ç›®å½•ã€‚ |
| `--save_dir` | `/path/to/moe_output` | ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè¾“å‡ºçš„ç›®å½•ã€‚ |
| `--from_scratch` | `False` | ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå³ä½¿å­˜åœ¨æ£€æŸ¥ç‚¹ä¹Ÿå¿½ç•¥ã€‚ |
| `--checkpoint_path` | `

ğŸ“œ License
This project is released under the MIT License.
